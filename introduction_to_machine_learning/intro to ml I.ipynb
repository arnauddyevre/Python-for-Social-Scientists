{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2HLo7LxSm0Wv"
   },
   "source": [
    "# Introduction to machine learning I\n",
    "\n",
    "Friday, September 27\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction_to_machine_learning/intro%20to%20ml%20I.ipynb)\n",
    "\n",
    "### 1. Marriage of machine learning and social science: Gebru et al. (2017)\n",
    "\n",
    "**1.1 Two social science questions**\n",
    "\n",
    "A social science question\n",
    "\n",
    "> **how to estimate the demographic makeup of neighborhoods across the United States?**\n",
    "\n",
    "and an even more interesting question\n",
    "\n",
    "> **how does the demographic makeup of neighborhoods affect the presidential election?**\n",
    "\n",
    "Social scientists might have been studying such problems for decades.\n",
    "And now another community also become interested in such problems and come with some new data, new idea and new methods.\n",
    "That is machine leanring community.\n",
    "\n",
    "_The images within the first section are from_ [Timnit Gebru's presentation slides on Machine Learning Summer School 2019](https://github.com/mlss-2019/slides/blob/master/fairness/fairness_slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_RKK7s6m0W0"
   },
   "source": [
    "**1.2. Answers from machine learning researchers**\n",
    "\n",
    "Answer to the above questions from some machine learning researchers in Stanford:\n",
    "\n",
    "> **Cars**\n",
    "\n",
    "\n",
    "Why?\n",
    "\n",
    "* *Popularity*: cars are everywhere in every district of the United States\n",
    "* *Diversity*: cars are diverse enough to reflect the different taste of people (saving/consuming, altitute to foreign countries, sensitivity to the income change, etc)\n",
    "\n",
    "Next questions: **how to get data about cars?**\n",
    "\n",
    "A naive answer would be a census of the cars of people in every district of USA which will cost huge money and resources. (every year USA spends over 1 billion dollars on census). \n",
    "\n",
    "Another answer\n",
    "> use the **auto-detection** and **classification** techniques from machine learning and computer vision\n",
    "\n",
    "more specifically,\n",
    "> Using **deep learning-based computer vision techniques** to estimate socioeconomic characteristics of regions spanning 200 US cities by using 50 million images of street scenes gathered with Google Street View cars\n",
    "\n",
    "![Google View](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction_to_machine_learning/figures/car.jpg?raw=1)\n",
    "\n",
    "_Source from_: [Timnit Gebru's presentation slides on Machine Learning Summer School 2019](https://github.com/mlss-2019/slides/blob/master/fairness/fairness_slides.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "telVhT03m0W2"
   },
   "source": [
    "**1.3. The effect of machine learning techniques**\n",
    "\n",
    "Gebru et al. (2017) create a data set of cars in the 50 million Google view images using deep learning techniques. It is free to download here [Visual Census: Fine-Grained Car Dataset](https://ai.stanford.edu/~tgebru/car_data.html)\n",
    "\n",
    "***Q1: How green is each state?***\n",
    "\n",
    "![green](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction_to_machine_learning/figures/green.jpg?raw=1)\n",
    "\n",
    "***Q2: Can we predict the income?***\n",
    "\n",
    "![income](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction_to_machine_learning/figures/income.jpg?raw=1)\n",
    "\n",
    "***Q3: Can we predict the race distribution?***\n",
    "\n",
    "![race](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction_to_machine_learning/figures/race.jpg?raw=1)\n",
    "\n",
    "***Q4: Can we predict the voting pattern?***\n",
    "\n",
    "![vote](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction_to_machine_learning/figures/vote.jpg?raw=1)\n",
    "\n",
    "_Source from_: [Timnit Gebru's presentation slides on Machine Learning Summer School 2019](https://github.com/mlss-2019/slides/blob/master/fairness/fairness_slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BeHTE1dYm0W4"
   },
   "source": [
    "Now we will study the techniques behind such great success: **supervised learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H921eFOum0W7"
   },
   "source": [
    "### 2. Supervised learning\n",
    "\n",
    "> Supervised learning is the machine learning task of inferring a function from labeled training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCfZ2NGwm0W-"
   },
   "source": [
    "#### 2.1. Empirical Risk Minimization (with penalty)\n",
    "\n",
    "Mathematically, it means there is an unobservable function $f: \\mathcal{X} \\rightarrow Y$ where  $\\mathcal{X} $ is the input space, $\\mathcal{Y}$ is the label space. And the goal of the supervised learning is to output an estimate of the function $\\hat{f}$ given the data $(X_1, Y_1), \\dots, (X_n, Y_n) \\overset{i.i.d}{\\sim}  \\mathcal{X} \\times  \\mathcal{Y}$\n",
    "\n",
    "1. We need to specify a candidate set $\\mathcal{F}$ of all the potential estimators $g$ lives in.\n",
    "\n",
    "2. For the candidate set $\\mathcal{F}$, we should specify a complexity measure $\\mathcal{C}(g) \\in \\mathbb{R}$ for each function $g(\\cdot) \\in \\mathcal{F}$.\n",
    "A more complex $g$ should have larger $\\mathcal{C}(g)$.\n",
    "\n",
    "2. We will define a loss function $l(g(X_i), Y_i)$ on the estimated value $g(X_i)$ and the true value $Y_i$ for all observed data $i=1,\\dots,n$ \n",
    "\n",
    "3. Then we minimize the sum of *empirical risk*, the sum of the losses $l(g(X_i), Y_i)$, and a penalty term $\\mathcal{C}(g)$, which penalizes the more complex more.\n",
    "The minimization process at the end will output a minimizer as the estimator $\\hat{f}$ for the ground truth function $f$\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{g \\in \\mathcal{F}}{\\text{minimize}}\n",
    "& & \\sum_{i=1}^{n} l(g(X_i), Y_i) + \\lambda \\mathcal{C}(g)\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $\\lambda$ is a hyperparameter characterizing the ratio between the empirical risk and the penalty.\n",
    "\n",
    "Examples:\n",
    "\n",
    "| Algorithms | $\\mathcal{X}$ | $\\mathcal{Y}$ | $\\mathcal{F}$ | $l(g(X_i), Y_i)$ | $\\mathcal{C}(g)$|\n",
    "|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| linear regression | $\\mathbb{R}^d$ | $\\mathbb{R}$ | $\\{X\\beta| \\beta \\in \\mathbb{R}^d\\}$ | $\\left[g(X_i) - Y_i\\right]^2$ | $||\\beta||$ |\n",
    "| logistic regression | $\\mathbb{R}^d$ | $\\{0, 1\\}$ | $\\{\\sigma(X\\beta)| \\beta \\in \\mathbb{R}^d\\}$ | $-Y_i\\log\\left[\\sigma(X_i\\beta)\\right] - (1-Y_i)\\log\\left[1-\\sigma(X_i\\beta)\\right]$ | $||\\beta||$ |\n",
    "| support vector machine | $\\mathbb{R}^d$ | $\\{0, 1\\}$ | $\\{w_{\\theta}(X)| \\theta \\in \\mathbb{R}^m\\}$ | $\\max\\left(0, 1- Y_i w_{\\theta}(X_i)\\right)$ | $||\\theta||$ |\n",
    "| $L$-layered neural network | $\\mathbb{R}^d$ | $\\{0, 1\\}$ | $\\{h_{\\theta_L}\\circ\\dots\\circ h_{\\theta_1}(X)| \\theta \\in \\mathbb{R}^m\\}$ | $-Y_i\\log\\left[\\sigma(X_i\\beta)\\right] - (1-Y_i)\\log\\left[1-\\sigma(X_i\\beta)\\right]$ | $ \\sum_{l=1}^L ||\\theta_l||$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmyY0NQ6JLkc"
   },
   "source": [
    "#### 2.2 Optimisation for Machine Learning\n",
    "\n",
    "The method that most people in machine learning community use is *stochastic gradient descent*.\n",
    "\n",
    "Suppose $\\mathcal{F} = \\{g_\\theta | \\theta \\in \\Theta\\}$ and $g$ is differentiable with respect to $\\theta$.\n",
    "\n",
    "---\n",
    "\n",
    "Stochastic Gradient Descent (SGD)\n",
    "\n",
    "---\n",
    "\n",
    "While not stop {\n",
    "\n",
    "> sample a smaller sub data set of size $r$ from the full data set with replacement, denote it as $\\left[(X_{i_1}, Y_{i_1}), \\dots, (X_{i_r}, Y_{i_r})\\right]$, called *mini-batch*\n",
    "\n",
    "> updating the parameter $\\theta$ based on minimizing the empirical loss and penaty\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\sum_{s=1}^{r} \\frac{\\partial l(g_\\theta(X_{i_s}), Y_{i_s})}{\\partial \\theta} - \\lambda\\frac{\\partial C(g_\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "SGD is used because:\n",
    "\n",
    "1. **scalability**: it only needs graduate whose computation complexity is linear in the number of parameters\n",
    "2. **statistical efficiency**: SGD could find the \"good\" local optimal for non-convex empirical risk\n",
    "3. **simplicity**: the algorithm could be parallelled in a simple way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ESPRiEHOm0XB"
   },
   "source": [
    "#### 2.3. Model Evaluation\n",
    "\n",
    "In machine learning, we want our algorithms could output a estimator that approximates the ground truth function well even on the data we have not seen --- it's called generalization.\n",
    "\n",
    "**Data set division**\n",
    "\n",
    "First we will divide the data set into 2 parts\n",
    "\n",
    "* _training data_ : learning the estimator $\\hat{f}$ (sometimes, it will also used to choose the hyperparameters, e.g. $\\lambda$)\n",
    "* _testing data_ :  evaluating the generalizaiton ability of $\\hat{f}$\n",
    "\n",
    "\n",
    "**Prediction accuracy**\n",
    "\n",
    "Supervised learning can be divided into two catergories based on the output of this function:\n",
    "* regression problem: $Y \\in \\mathbb{R}$:\n",
    "$$\n",
    "\\frac{1}{|\\text{testing data}|}\\sum_{j\\in \\text{testing data}} l(g(X_j), Y_j)\n",
    "$$\n",
    "* classfication problem: $Y \\in \\{0, 1, \\dots, n\\}$\n",
    "$$\n",
    "\\frac{1}{|\\text{testing data}|}\\sum_{j\\in \\text{testing data}} 1(g(X_j) = Y_j)\n",
    "$$\n",
    "\n",
    "**Generalisation error and overfitting**\n",
    "\n",
    "![overfitting](https://raw.githubusercontent.com/DS-100/textbook/master/assets/feature_train_test_error.png)\n",
    "\n",
    "_Source from_: [overfitting](https://raw.githubusercontent.com/DS-100/textbook/master/assets/feature_train_test_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM_-Cwhvm0XI"
   },
   "source": [
    "There have been many methods nowadays:\n",
    "* linear regression\n",
    "* support vector machine\n",
    "* random forest\n",
    "* gradient boosting machine\n",
    "* neural network (aka deep learning)\n",
    "\n",
    "all these methods can be found in `sklearn` package which is the one of the most popular machine learning packages in Python.\n",
    "\n",
    "Anaconda has installed `sklearn` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yiBjZZdjm0XK"
   },
   "source": [
    "### 3. Typical machine learning algorithms in `sklearn`\n",
    "\n",
    "**3.1 Data set**\n",
    "\n",
    "we use the `Guerry` data set from `statsmodels` to study a regression problem: can we predict the sales of the lottery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8rEBmACm0XM",
    "outputId": "8d8ffae0-3ab0-4e2f-d5d0-0bd955b3d54a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crime_pers</th>\n",
       "      <th>Crime_prop</th>\n",
       "      <th>Literacy</th>\n",
       "      <th>Donations</th>\n",
       "      <th>Infants</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Commerce</th>\n",
       "      <th>Clergy</th>\n",
       "      <th>Crime_parents</th>\n",
       "      <th>Infanticide</th>\n",
       "      <th>Donation_clergy</th>\n",
       "      <th>Lottery</th>\n",
       "      <th>Desertion</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Prostitutes</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Area</th>\n",
       "      <th>Pop1831</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28870</td>\n",
       "      <td>15890</td>\n",
       "      <td>37</td>\n",
       "      <td>5098</td>\n",
       "      <td>33120</td>\n",
       "      <td>35039</td>\n",
       "      <td>73</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>71</td>\n",
       "      <td>60</td>\n",
       "      <td>69</td>\n",
       "      <td>41</td>\n",
       "      <td>55</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>218.372</td>\n",
       "      <td>5762</td>\n",
       "      <td>346.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26226</td>\n",
       "      <td>5521</td>\n",
       "      <td>51</td>\n",
       "      <td>8901</td>\n",
       "      <td>14572</td>\n",
       "      <td>12831</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>82</td>\n",
       "      <td>24</td>\n",
       "      <td>327</td>\n",
       "      <td>65.945</td>\n",
       "      <td>7369</td>\n",
       "      <td>513.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26747</td>\n",
       "      <td>7925</td>\n",
       "      <td>13</td>\n",
       "      <td>10973</td>\n",
       "      <td>17044</td>\n",
       "      <td>114121</td>\n",
       "      <td>61</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>46</td>\n",
       "      <td>42</td>\n",
       "      <td>76</td>\n",
       "      <td>66</td>\n",
       "      <td>16</td>\n",
       "      <td>85</td>\n",
       "      <td>34</td>\n",
       "      <td>161.927</td>\n",
       "      <td>7340</td>\n",
       "      <td>298.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12935</td>\n",
       "      <td>7289</td>\n",
       "      <td>46</td>\n",
       "      <td>2733</td>\n",
       "      <td>23018</td>\n",
       "      <td>14238</td>\n",
       "      <td>76</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>351.399</td>\n",
       "      <td>6925</td>\n",
       "      <td>155.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17488</td>\n",
       "      <td>8174</td>\n",
       "      <td>69</td>\n",
       "      <td>6962</td>\n",
       "      <td>23076</td>\n",
       "      <td>16171</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>64</td>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>320.280</td>\n",
       "      <td>5549</td>\n",
       "      <td>129.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Crime_pers  Crime_prop  Literacy  Donations  Infants  Suicides  Wealth  \\\n",
       "0       28870       15890        37       5098    33120     35039      73   \n",
       "1       26226        5521        51       8901    14572     12831      22   \n",
       "2       26747        7925        13      10973    17044    114121      61   \n",
       "3       12935        7289        46       2733    23018     14238      76   \n",
       "4       17488        8174        69       6962    23076     16171      83   \n",
       "\n",
       "   Commerce  Clergy  Crime_parents  Infanticide  Donation_clergy  Lottery  \\\n",
       "0        58      11             71           60               69       41   \n",
       "1        10      82              4           82               36       38   \n",
       "2        66      68             46           42               76       66   \n",
       "3        49       5             70           12               37       80   \n",
       "4        65      10             22           23               64       79   \n",
       "\n",
       "   Desertion  Instruction  Prostitutes  Distance  Area  Pop1831  \n",
       "0         55           46           13   218.372  5762   346.03  \n",
       "1         82           24          327    65.945  7369   513.00  \n",
       "2         16           85           34   161.927  7340   298.26  \n",
       "3         32           29            2   351.399  6925   155.90  \n",
       "4         35            7            1   320.280  5549   129.10  "
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "dat = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n",
    "# remove all the non-numerical columns\n",
    "dat.drop([\"dept\", \"Region\", \"Department\", \"MainCity\"], axis=1, inplace=True)\n",
    "# list of the variables\n",
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNMwVQS5m0XU"
   },
   "source": [
    "The detailed information about the data set can be found [here](https://rdata.pmagunia.com/dataset/r-dataset-package-histdata-guerry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUkaGQM0m0XV",
    "outputId": "42eefd1a-730c-48ea-ad10-3a0935fc06c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crime_pers</th>\n",
       "      <th>Crime_prop</th>\n",
       "      <th>Literacy</th>\n",
       "      <th>Donations</th>\n",
       "      <th>Infants</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Commerce</th>\n",
       "      <th>Clergy</th>\n",
       "      <th>Crime_parents</th>\n",
       "      <th>Infanticide</th>\n",
       "      <th>Donation_clergy</th>\n",
       "      <th>Lottery</th>\n",
       "      <th>Desertion</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Prostitutes</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Area</th>\n",
       "      <th>Pop1831</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19754.406977</td>\n",
       "      <td>7843.058140</td>\n",
       "      <td>39.255814</td>\n",
       "      <td>7075.546512</td>\n",
       "      <td>19049.906977</td>\n",
       "      <td>36522.604651</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>42.802326</td>\n",
       "      <td>43.430233</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.511628</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.127907</td>\n",
       "      <td>141.872093</td>\n",
       "      <td>207.953140</td>\n",
       "      <td>6146.988372</td>\n",
       "      <td>378.628721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7504.703073</td>\n",
       "      <td>3051.352839</td>\n",
       "      <td>17.364051</td>\n",
       "      <td>5834.595216</td>\n",
       "      <td>8820.233546</td>\n",
       "      <td>31312.532649</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>25.028370</td>\n",
       "      <td>24.999549</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.948297</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.799809</td>\n",
       "      <td>520.969318</td>\n",
       "      <td>109.320837</td>\n",
       "      <td>1398.246620</td>\n",
       "      <td>148.777230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2199.000000</td>\n",
       "      <td>1368.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1246.000000</td>\n",
       "      <td>2660.000000</td>\n",
       "      <td>3460.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>129.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14156.250000</td>\n",
       "      <td>5933.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3446.750000</td>\n",
       "      <td>14299.750000</td>\n",
       "      <td>15463.000000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>121.383000</td>\n",
       "      <td>5400.750000</td>\n",
       "      <td>283.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18748.500000</td>\n",
       "      <td>7595.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>5020.000000</td>\n",
       "      <td>17141.500000</td>\n",
       "      <td>26743.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>200.616000</td>\n",
       "      <td>6070.500000</td>\n",
       "      <td>346.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25937.500000</td>\n",
       "      <td>9182.250000</td>\n",
       "      <td>51.750000</td>\n",
       "      <td>9446.750000</td>\n",
       "      <td>22682.250000</td>\n",
       "      <td>44057.500000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>113.750000</td>\n",
       "      <td>289.670500</td>\n",
       "      <td>6816.500000</td>\n",
       "      <td>444.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>37014.000000</td>\n",
       "      <td>20235.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>37015.000000</td>\n",
       "      <td>62486.000000</td>\n",
       "      <td>163241.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4744.000000</td>\n",
       "      <td>539.213000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>989.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Crime_pers    Crime_prop   Literacy     Donations       Infants  \\\n",
       "count     86.000000     86.000000  86.000000     86.000000     86.000000   \n",
       "mean   19754.406977   7843.058140  39.255814   7075.546512  19049.906977   \n",
       "std     7504.703073   3051.352839  17.364051   5834.595216   8820.233546   \n",
       "min     2199.000000   1368.000000  12.000000   1246.000000   2660.000000   \n",
       "25%    14156.250000   5933.000000  25.000000   3446.750000  14299.750000   \n",
       "50%    18748.500000   7595.000000  38.000000   5020.000000  17141.500000   \n",
       "75%    25937.500000   9182.250000  51.750000   9446.750000  22682.250000   \n",
       "max    37014.000000  20235.000000  74.000000  37015.000000  62486.000000   \n",
       "\n",
       "            Suicides     Wealth   Commerce     Clergy  Crime_parents  \\\n",
       "count      86.000000  86.000000  86.000000  86.000000      86.000000   \n",
       "mean    36522.604651  43.500000  42.802326  43.430233      43.500000   \n",
       "std     31312.532649  24.969982  25.028370  24.999549      24.969982   \n",
       "min      3460.000000   1.000000   1.000000   1.000000       1.000000   \n",
       "25%     15463.000000  22.250000  21.250000  22.250000      22.250000   \n",
       "50%     26743.500000  43.500000  42.500000  43.500000      43.500000   \n",
       "75%     44057.500000  64.750000  63.750000  64.750000      64.750000   \n",
       "max    163241.000000  86.000000  86.000000  86.000000      86.000000   \n",
       "\n",
       "       Infanticide  Donation_clergy    Lottery  Desertion  Instruction  \\\n",
       "count    86.000000        86.000000  86.000000  86.000000    86.000000   \n",
       "mean     43.511628        43.500000  43.500000  43.500000    43.127907   \n",
       "std      24.948297        24.969982  24.969982  24.969982    24.799809   \n",
       "min       1.000000         1.000000   1.000000   1.000000     1.000000   \n",
       "25%      22.250000        22.250000  22.250000  22.250000    23.250000   \n",
       "50%      43.500000        43.500000  43.500000  43.500000    41.500000   \n",
       "75%      64.750000        64.750000  64.750000  64.750000    64.750000   \n",
       "max      86.000000        86.000000  86.000000  86.000000    86.000000   \n",
       "\n",
       "       Prostitutes    Distance          Area     Pop1831  \n",
       "count    86.000000   86.000000     86.000000   86.000000  \n",
       "mean    141.872093  207.953140   6146.988372  378.628721  \n",
       "std     520.969318  109.320837   1398.246620  148.777230  \n",
       "min       0.000000    0.000000    762.000000  129.100000  \n",
       "25%       6.000000  121.383000   5400.750000  283.005000  \n",
       "50%      33.000000  200.616000   6070.500000  346.165000  \n",
       "75%     113.750000  289.670500   6816.500000  444.407500  \n",
       "max    4744.000000  539.213000  10000.000000  989.940000  "
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptneX7OUm0Xa"
   },
   "outputs": [],
   "source": [
    "Y = dat['Lottery']\n",
    "dat.drop('Lottery', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lY_J3CD4m0Xf"
   },
   "source": [
    "We randomly divide the data set into `train_X`, `train_Y` , `test_X` and `test_Y` where the size of training, validation and test  $\\sim 4:1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5axtkU6m0Xh",
    "outputId": "8f2c4a45-a348-45fc-9046-42cdaee77756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 70 training data\n",
      "There are 16 test data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "test_X = [] \n",
    "test_Y = []\n",
    "\n",
    "for idx, row in dat.iterrows():\n",
    "    r = np.random.randint(5)\n",
    "    if  r < 4:\n",
    "        train_X.append(row.values)\n",
    "        train_Y.append(Y[idx])\n",
    "    else:\n",
    "        test_X.append(row.values)\n",
    "        test_Y.append(Y[idx])\n",
    "\n",
    "print(\"There are \" + str(len(train_Y)) + \" training data\")\n",
    "print(\"There are \" + str(len(test_Y)) + \" test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQjAKRO6m0Xm"
   },
   "source": [
    "**3.2. Linear model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_DZ-FsHm0Xm",
    "outputId": "b8241d9e-7b22-4563-899a-1b83e52881ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 428.82\n",
      "Variance score: 0.42\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(train_X, train_Y)\n",
    "ols_pred_Y = ols.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, ols_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, ols_pred_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HaihJLFAm0Xs",
    "outputId": "009e027f-c970-4b2c-f18d-b5777ee57f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 524.26\n",
      "Variance score: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "lasso = LassoCV(cv=5, random_state=0)\n",
    "lasso.fit(train_X, train_Y)\n",
    "lasso_pred_Y = lasso.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, lasso_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, lasso_pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUtlJqzem0Xy"
   },
   "source": [
    "**3.3. $k$-Nearest Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1a1ZBqvLm0X0",
    "outputId": "5427628f-c097-4bf6-bf4b-a140f1b8a7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 587.54\n",
      "Variance score: 0.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "n_neighbors = 10\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "knn.fit(train_X, train_Y)\n",
    "knn_pred_Y = knn.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, knn_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, knn_pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V036ltb1m0X3"
   },
   "source": [
    "**3.4. Support vector regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o76RGS6lm0X4",
    "outputId": "b2b6eb56-e48a-4c06-d54b-78cdb77316e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 771.00\n",
      "Variance score: -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR()\n",
    "clf.fit(train_X, train_Y)\n",
    "clf_pred_Y = clf.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, clf_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, clf_pred_Y))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of intro to ml I.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
