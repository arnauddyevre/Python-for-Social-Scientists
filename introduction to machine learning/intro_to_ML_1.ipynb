{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning I\n",
    "\n",
    "Thursday, September 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "\n",
    "- [1. Machine learning and social science: an example](#1.-Machine-learning-and-social-science:-an-example)\n",
    "- [2. Supervised learning](#2.-Supervised-learning)\n",
    "- [3. Typical machine learning algorithms in sklearn](#3.-Typical-machine-learning-algorithms-in-sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will... **[TBC]**\n",
    "\n",
    "\n",
    "### 1. Machine learning and social science: an example\n",
    "\n",
    "We will use a recently published paper [Gebru et al. (2017)](https://www.pnas.org/content/114/50/13108), written by Stanford computer scientists, to show how machine learning techniques can be used to deliver powerful insights for social scientists.\n",
    "\n",
    "#### 1.1 Two social science questions\n",
    "\n",
    "Demographers and political scientists would typically be interested in the following questions:\n",
    "\n",
    "> **how to estimate the demographic makeup of neighborhoods across the United States?**\n",
    "\n",
    "> **how does the demographic makeup of neighborhoods affect the presidential election?**\n",
    "\n",
    "Social scientists might have been studying such problems for decades. The machine learning community has found creative ways to tackle these questions. It has come up with new data, new idea and new methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. Answers from machine learning researchers**\n",
    "\n",
    "The surprising answer given by the group of Stanford scholar is:\n",
    "\n",
    "> **Cars**\n",
    "\n",
    "\n",
    "Why?\n",
    "\n",
    "* *Popularity*: cars are everywhere in every district of the United States, and they are easily observable in public spaces\n",
    "* *Diversity*: cars are diverse enough to reflect the different taste of people (saving/consumption, attitude toward foreign goods, sensitivity to income changes, etc.)\n",
    "\n",
    "Next questions: **how to get data about cars?**\n",
    "\n",
    "A naive answer would be a census of the cars of people in every district of USA which will cost huge money and resources. (every year, the US federal government spends over $1 billion on the census). \n",
    "\n",
    "Another answer, taking advantage of recent advances in machine learning:\n",
    "> Use the **auto-detection** and **classification** techniques from machine learning and computer vision\n",
    "\n",
    "More specifically,\n",
    "> Leverage **deep learning-based computer vision techniques** to estimate socioeconomic characteristics of neighbourhoods across the US by using 50 million images of street scenes gathered with Google Street View cars\n",
    "\n",
    "![Google View](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/car.jpg?raw=true)\n",
    "\n",
    "**[AD: we need a source for this image]**\n",
    "\n",
    "#### 1.3. The effect of machine learning techniques\n",
    "\n",
    "Gebru et al. (2017) assemble a massive data set of 50 million Google View images, and identify all the cars in these images using deep learning techniques. It is free to download here [Visual Census: Fine-Grained Car Dataset](https://ai.stanford.edu/~tgebru/car_data.html).\n",
    "\n",
    "Based solely on the distributions of car models across US neighbourhoods, the authors manage to provide answers to the following questions:\n",
    "\n",
    "***Q1: How green is each state?***\n",
    "\n",
    "![green](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/green.jpg?raw=true)\n",
    "\n",
    "***Q2: Can we predict average income?***\n",
    "\n",
    "![income](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/income.jpg?raw=true)\n",
    "\n",
    "***Q3: Can we predict the race distribution?***\n",
    "\n",
    "![race](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/race.jpg?raw=true)\n",
    "\n",
    "***Q4: Can we predict voting patterns?***\n",
    "\n",
    "![vote](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/vote.jpg?raw=true)\n",
    "\n",
    "We study below the techniques behind their success in predicting important socio-demographic variables with such precision, and geographical granularity: all of them fall in the class of **supervised learning** algorithms.\n",
    "\n",
    "### 2. Supervised learning\n",
    "\n",
    "> Supervised learning is the machine learning task of inferring a function from labeled training data.\n",
    "\n",
    "With supervised learning problems, we observe both the features $X_i$ of an observation and its label, or outcome $Y_i$.\n",
    "\n",
    "#### 2.1. Mathematical definition\n",
    "\n",
    "Mathematically, it means there is an unobservable function $f: X_i \\rightarrow Y_i$ where  $X_i$ is the input, $Y_i$ is the label.\n",
    "Given the data $(X_1, Y_1), \\dots, (X_n, Y_n)$, we want to use an algorithm to output an estimate of the function $\\hat{f}$.\n",
    "\n",
    "#### 2.2. Basic ideas\n",
    "\n",
    "**Training, testing data**\n",
    "\n",
    "Usually, we will divide the data set into 3 parts **[2 parts right? or is there a third missing below?]**: \n",
    "\n",
    "* _training data_ : learn the parameters of the model (sometimes also called hyperparameters **[TBC: When are these parameters called hyperparameters?]**)\n",
    "* _testing data_ : estimate the performance of the model\n",
    "\n",
    "**Prediction accuracy**\n",
    "\n",
    "Supervised learning can be divided into two catergories based on the output of this function:\n",
    "\n",
    "* regression problem: $Y \\in \\mathbb{R}$\n",
    "* classfication problem: $Y \\in \\{0, 1, \\dots, n\\}$\n",
    "\n",
    "In machine learning, the prediction accuaracy is the mean squared error between the predicted values and the real values in the regression problem and the classification correctness rate for the classification problem. **[TBC: we need to add formulas for the mean-squared error and the classification correctness rate here]**\n",
    "\n",
    "**Generalisation error and overfitting**\n",
    "\n",
    "**[an explanation would be welcome here, the picture seems to be in vacuum here. Also, we need to make sure that pictures are properly referenced. A source should be added]**\n",
    "\n",
    "![overfitting](https://raw.githubusercontent.com/DS-100/textbook/master/assets/feature_train_test_error.png)\n",
    "\n",
    "Machine learning is a broad term, that encompass many techniques:\n",
    "* nearest neighbor and clustering algorithms\n",
    "* naive Bayes\n",
    "* kernel functions\n",
    "* support vector machine\n",
    "* random forest\n",
    "* (stochastic) gradient descent\n",
    "* gradient boosting machine\n",
    "* neural network (aka deep learning)\n",
    "\n",
    "All these methods can be found in `sklearn` package which is the most popular machine learning packages in Python.\n",
    "\n",
    "Anaconda already has the `sklearn` module installed. Hence to use it, we simply need to import it at the beginning of our Notebook.\n",
    "\n",
    "### 3. Typical machine learning algorithms in sklearn\n",
    "\n",
    "#### 3.1 Data set\n",
    "\n",
    "We use the `Guerry` data set from `statsmodels` to study a regression problem: can we predict the sales of the lottery? The detailed information about the data set can be found [here](https://rdata.pmagunia.com/dataset/r-dataset-package-histdata-guerry) **[Similarly, it would be useful to give afew words of explanations about this dataset and where it comes from. Or why you chose it.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crime_pers</th>\n",
       "      <th>Crime_prop</th>\n",
       "      <th>Literacy</th>\n",
       "      <th>Donations</th>\n",
       "      <th>Infants</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Commerce</th>\n",
       "      <th>Clergy</th>\n",
       "      <th>Crime_parents</th>\n",
       "      <th>Infanticide</th>\n",
       "      <th>Donation_clergy</th>\n",
       "      <th>Lottery</th>\n",
       "      <th>Desertion</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Prostitutes</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Area</th>\n",
       "      <th>Pop1831</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28870</td>\n",
       "      <td>15890</td>\n",
       "      <td>37</td>\n",
       "      <td>5098</td>\n",
       "      <td>33120</td>\n",
       "      <td>35039</td>\n",
       "      <td>73</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>71</td>\n",
       "      <td>60</td>\n",
       "      <td>69</td>\n",
       "      <td>41</td>\n",
       "      <td>55</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "      <td>218.372</td>\n",
       "      <td>5762</td>\n",
       "      <td>346.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26226</td>\n",
       "      <td>5521</td>\n",
       "      <td>51</td>\n",
       "      <td>8901</td>\n",
       "      <td>14572</td>\n",
       "      <td>12831</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>82</td>\n",
       "      <td>24</td>\n",
       "      <td>327</td>\n",
       "      <td>65.945</td>\n",
       "      <td>7369</td>\n",
       "      <td>513.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26747</td>\n",
       "      <td>7925</td>\n",
       "      <td>13</td>\n",
       "      <td>10973</td>\n",
       "      <td>17044</td>\n",
       "      <td>114121</td>\n",
       "      <td>61</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>46</td>\n",
       "      <td>42</td>\n",
       "      <td>76</td>\n",
       "      <td>66</td>\n",
       "      <td>16</td>\n",
       "      <td>85</td>\n",
       "      <td>34</td>\n",
       "      <td>161.927</td>\n",
       "      <td>7340</td>\n",
       "      <td>298.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12935</td>\n",
       "      <td>7289</td>\n",
       "      <td>46</td>\n",
       "      <td>2733</td>\n",
       "      <td>23018</td>\n",
       "      <td>14238</td>\n",
       "      <td>76</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>80</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>351.399</td>\n",
       "      <td>6925</td>\n",
       "      <td>155.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17488</td>\n",
       "      <td>8174</td>\n",
       "      <td>69</td>\n",
       "      <td>6962</td>\n",
       "      <td>23076</td>\n",
       "      <td>16171</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>64</td>\n",
       "      <td>79</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>320.280</td>\n",
       "      <td>5549</td>\n",
       "      <td>129.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Crime_pers  Crime_prop  Literacy  Donations  Infants  Suicides  Wealth  \\\n",
       "0       28870       15890        37       5098    33120     35039      73   \n",
       "1       26226        5521        51       8901    14572     12831      22   \n",
       "2       26747        7925        13      10973    17044    114121      61   \n",
       "3       12935        7289        46       2733    23018     14238      76   \n",
       "4       17488        8174        69       6962    23076     16171      83   \n",
       "\n",
       "   Commerce  Clergy  Crime_parents  Infanticide  Donation_clergy  Lottery  \\\n",
       "0        58      11             71           60               69       41   \n",
       "1        10      82              4           82               36       38   \n",
       "2        66      68             46           42               76       66   \n",
       "3        49       5             70           12               37       80   \n",
       "4        65      10             22           23               64       79   \n",
       "\n",
       "   Desertion  Instruction  Prostitutes  Distance  Area  Pop1831  \n",
       "0         55           46           13   218.372  5762   346.03  \n",
       "1         82           24          327    65.945  7369   513.00  \n",
       "2         16           85           34   161.927  7340   298.26  \n",
       "3         32           29            2   351.399  6925   155.90  \n",
       "4         35            7            1   320.280  5549   129.10  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "dat = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n",
    "# remove all the non-numerical columns\n",
    "dat.drop([\"dept\", \"Region\", \"Department\", \"MainCity\"], axis=1, inplace=True)\n",
    "# list of the variables\n",
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crime_pers</th>\n",
       "      <th>Crime_prop</th>\n",
       "      <th>Literacy</th>\n",
       "      <th>Donations</th>\n",
       "      <th>Infants</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Commerce</th>\n",
       "      <th>Clergy</th>\n",
       "      <th>Crime_parents</th>\n",
       "      <th>Infanticide</th>\n",
       "      <th>Donation_clergy</th>\n",
       "      <th>Lottery</th>\n",
       "      <th>Desertion</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Prostitutes</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Area</th>\n",
       "      <th>Pop1831</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19754.406977</td>\n",
       "      <td>7843.058140</td>\n",
       "      <td>39.255814</td>\n",
       "      <td>7075.546512</td>\n",
       "      <td>19049.906977</td>\n",
       "      <td>36522.604651</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>42.802326</td>\n",
       "      <td>43.430233</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.511628</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.127907</td>\n",
       "      <td>141.872093</td>\n",
       "      <td>207.953140</td>\n",
       "      <td>6146.988372</td>\n",
       "      <td>378.628721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7504.703073</td>\n",
       "      <td>3051.352839</td>\n",
       "      <td>17.364051</td>\n",
       "      <td>5834.595216</td>\n",
       "      <td>8820.233546</td>\n",
       "      <td>31312.532649</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>25.028370</td>\n",
       "      <td>24.999549</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.948297</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.799809</td>\n",
       "      <td>520.969318</td>\n",
       "      <td>109.320837</td>\n",
       "      <td>1398.246620</td>\n",
       "      <td>148.777230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2199.000000</td>\n",
       "      <td>1368.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1246.000000</td>\n",
       "      <td>2660.000000</td>\n",
       "      <td>3460.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>129.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14156.250000</td>\n",
       "      <td>5933.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3446.750000</td>\n",
       "      <td>14299.750000</td>\n",
       "      <td>15463.000000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>121.383000</td>\n",
       "      <td>5400.750000</td>\n",
       "      <td>283.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18748.500000</td>\n",
       "      <td>7595.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>5020.000000</td>\n",
       "      <td>17141.500000</td>\n",
       "      <td>26743.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>200.616000</td>\n",
       "      <td>6070.500000</td>\n",
       "      <td>346.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25937.500000</td>\n",
       "      <td>9182.250000</td>\n",
       "      <td>51.750000</td>\n",
       "      <td>9446.750000</td>\n",
       "      <td>22682.250000</td>\n",
       "      <td>44057.500000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>113.750000</td>\n",
       "      <td>289.670500</td>\n",
       "      <td>6816.500000</td>\n",
       "      <td>444.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>37014.000000</td>\n",
       "      <td>20235.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>37015.000000</td>\n",
       "      <td>62486.000000</td>\n",
       "      <td>163241.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4744.000000</td>\n",
       "      <td>539.213000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>989.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Crime_pers    Crime_prop   Literacy     Donations       Infants  \\\n",
       "count     86.000000     86.000000  86.000000     86.000000     86.000000   \n",
       "mean   19754.406977   7843.058140  39.255814   7075.546512  19049.906977   \n",
       "std     7504.703073   3051.352839  17.364051   5834.595216   8820.233546   \n",
       "min     2199.000000   1368.000000  12.000000   1246.000000   2660.000000   \n",
       "25%    14156.250000   5933.000000  25.000000   3446.750000  14299.750000   \n",
       "50%    18748.500000   7595.000000  38.000000   5020.000000  17141.500000   \n",
       "75%    25937.500000   9182.250000  51.750000   9446.750000  22682.250000   \n",
       "max    37014.000000  20235.000000  74.000000  37015.000000  62486.000000   \n",
       "\n",
       "            Suicides     Wealth   Commerce     Clergy  Crime_parents  \\\n",
       "count      86.000000  86.000000  86.000000  86.000000      86.000000   \n",
       "mean    36522.604651  43.500000  42.802326  43.430233      43.500000   \n",
       "std     31312.532649  24.969982  25.028370  24.999549      24.969982   \n",
       "min      3460.000000   1.000000   1.000000   1.000000       1.000000   \n",
       "25%     15463.000000  22.250000  21.250000  22.250000      22.250000   \n",
       "50%     26743.500000  43.500000  42.500000  43.500000      43.500000   \n",
       "75%     44057.500000  64.750000  63.750000  64.750000      64.750000   \n",
       "max    163241.000000  86.000000  86.000000  86.000000      86.000000   \n",
       "\n",
       "       Infanticide  Donation_clergy    Lottery  Desertion  Instruction  \\\n",
       "count    86.000000        86.000000  86.000000  86.000000    86.000000   \n",
       "mean     43.511628        43.500000  43.500000  43.500000    43.127907   \n",
       "std      24.948297        24.969982  24.969982  24.969982    24.799809   \n",
       "min       1.000000         1.000000   1.000000   1.000000     1.000000   \n",
       "25%      22.250000        22.250000  22.250000  22.250000    23.250000   \n",
       "50%      43.500000        43.500000  43.500000  43.500000    41.500000   \n",
       "75%      64.750000        64.750000  64.750000  64.750000    64.750000   \n",
       "max      86.000000        86.000000  86.000000  86.000000    86.000000   \n",
       "\n",
       "       Prostitutes    Distance          Area     Pop1831  \n",
       "count    86.000000   86.000000     86.000000   86.000000  \n",
       "mean    141.872093  207.953140   6146.988372  378.628721  \n",
       "std     520.969318  109.320837   1398.246620  148.777230  \n",
       "min       0.000000    0.000000    762.000000  129.100000  \n",
       "25%       6.000000  121.383000   5400.750000  283.005000  \n",
       "50%      33.000000  200.616000   6070.500000  346.165000  \n",
       "75%     113.750000  289.670500   6816.500000  444.407500  \n",
       "max    4744.000000  539.213000  10000.000000  989.940000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dat['Lottery']\n",
    "dat.drop('Lottery', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly divide the data set into `train_X`, `train_Y` , `test_X` and `test_Y` where the size of training, validation and test  $\\sim 4:1$ **[We would need an explanation of how to divide the dataset in training and test data, what's the rule of thumb or the heuristic here?]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65 training data\n",
      "There are 21 test data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "test_X = [] \n",
    "test_Y = []\n",
    "\n",
    "for idx, row in dat.iterrows():\n",
    "    r = np.random.randint(5)\n",
    "    if  r < 4:\n",
    "        train_X.append(row.values)\n",
    "        train_Y.append(Y[idx])\n",
    "    else:\n",
    "        test_X.append(row.values)\n",
    "        test_Y.append(Y[idx])\n",
    "\n",
    "print(\"There are \" + str(len(train_Y)) + \" training data\")\n",
    "print(\"There are \" + str(len(test_Y)) + \" test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Linear model\n",
    "\n",
    "\n",
    "**[I think it would be helpful if a bit more details were added about the variance score and the mean-squared errors. Just to give abit of context around the cell below.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 428.82\n",
      "Variance score: 0.42\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(train_X, train_Y)\n",
    "ols_pred_Y = ols.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, ols_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, ols_pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[We need an explanation of what LASSO does and its mathematical expression: see my attempt below (to be completed)]**\n",
    "\n",
    "LASSO minimises the sum of squared residuals and a penalty term that increases with the dimension of the regression coefficient. In a sense, LASSO penalises model complexity.\n",
    "\n",
    "$$\\arg \\min _{\\beta} \\sum_{i=1}^{N}\\left(Y_{i}-\\beta^{\\top} X_{i}\\right)^{2}+\\lambda\\left(\\|\\beta\\|_{q}\\right)^{1 /q}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 524.26\n",
      "Variance score: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "lasso = LassoCV(cv=5, random_state=0)\n",
    "lasso.fit(train_X, train_Y)\n",
    "lasso_pred_Y = lasso.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, lasso_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, lasso_pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. $k$-Nearest Neighbor\n",
    "\n",
    "**[Here again, some explanations are necessary, both for SVM and nearest neighbours]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 587.54\n",
      "Variance score: 0.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "n_neighbors = 10\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "knn.fit(train_X, train_Y)\n",
    "knn_pred_Y = knn.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, knn_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, knn_pred_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 771.00\n",
      "Variance score: -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR()\n",
    "clf.fit(train_X, train_Y)\n",
    "clf_pred_Y = clf.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, clf_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, clf_pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[I think this Notebook is a bit too short...]**. It would be great if it were expanded with more techniques/methods. We should also add a class exercise with TensorFlow. One idea I have, which could be interesting for students, and would not require too much work from us would be to use this tutorial by TensorFlow (https://www.tensorflow.org/tutorials/keras/basic_text_classification), to build a neural network classifying movie reviews. The code is made available under an Apache licence, so we could re-use it in the class. We could also give students an overview of neuraol networks with this nice video series from 3Blue1Brown: https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**Gebru, T., Krause, J., Wang, Y., Chen, D., Deng, J., Aiden, E. L., & Fei-Fei, L.** (2017). Using deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States. *Proceedings of the National Academy of Sciences*, 114(50), 13108-13113.\n",
    "\n",
    "**Athey, S., & Imbens, G. W.** (2019). Machine learning methods that economists should know about. *Annual Review of Economics*, 11."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
