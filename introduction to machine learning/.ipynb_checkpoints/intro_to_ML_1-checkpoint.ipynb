{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning I\n",
    "\n",
    "Thursday, September 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "\n",
    "- [1. Machine learning and social science: an example](#1.-Machine-learning-and-social-science:-an-example)\n",
    "- [2. Coding the MLE estimator](#2.-Coding-the-MLE-estimator)\n",
    "- [3. MLE with 'statsmodels'](#3.-MLE-with-'statsmodels')\n",
    "- [4. Discrete choice models with 'statsmodels'](#4-Discrete-choice-models-with-'statsmodels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will... **[TBC]**\n",
    "\n",
    "\n",
    "### 1. Machine learning and social science: an example\n",
    "\n",
    "We will use a recently published paper [Gebru et al. (2017)](https://www.pnas.org/content/114/50/13108), written by Stanford computer scientists, to show how machine learning techniques can be used to deliver powerful insights for social scientists.\n",
    "\n",
    "#### 1.1 Two social science questions\n",
    "\n",
    "Demographers and political scientists would typically be interested in the following questions:\n",
    "\n",
    "> **how to estimate the demographic makeup of neighborhoods across the United States?**\n",
    "\n",
    "> **how does the demographic makeup of neighborhoods affect the presidential election?**\n",
    "\n",
    "Social scientists might have been studying such problems for decades. The machine learning community has found creative ways to tackle these questions. It has come up with new data, new idea and new methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. Answers from machine learning researchers**\n",
    "\n",
    "The surprising answer given by the group of Stanford scholar is:\n",
    "\n",
    "> **Cars**\n",
    "\n",
    "\n",
    "Why?\n",
    "\n",
    "* *Popularity*: cars are everywhere in every district of the United States, and they are easily observable in public spaces\n",
    "* *Diversity*: cars are diverse enough to reflect the different taste of people (saving/consumption, attitude toward foreign goods, sensitivity to income changes, etc.)\n",
    "\n",
    "Next questions: **how to get data about cars?**\n",
    "\n",
    "A naive answer would be a census of the cars of people in every district of USA which will cost huge money and resources. (every year, the US federal government spends over $1 billion on the census). \n",
    "\n",
    "Another answer, taking advantage of recent advances in machine learning:\n",
    "> Use the **auto-detection** and **classification** techniques from machine learning and computer vision\n",
    "\n",
    "More specifically,\n",
    "> Leverage **deep learning-based computer vision techniques** to estimate socioeconomic characteristics of neighbourhoods across the US by using 50 million images of street scenes gathered with Google Street View cars\n",
    "\n",
    "![Google View](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/car.jpg?raw=true)\n",
    "\n",
    "**[AD: we need a source for this image]**\n",
    "\n",
    "#### 1.3. The effect of machine learning techniques\n",
    "\n",
    "Gebru et al. (2017) assemble a massive data set of 50 million Google View images, and identify all the cars in these images using deep learning techniques. It is free to download here [Visual Census: Fine-Grained Car Dataset](https://ai.stanford.edu/~tgebru/car_data.html).\n",
    "\n",
    "Based solely on the distributions of car models across US neighbourhoods, the authors manage to provide answers to the following questions:\n",
    "\n",
    "***Q1: How green is each state?***\n",
    "\n",
    "![green](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/green.jpg?raw=true)\n",
    "\n",
    "***Q2: Can we predict average income?***\n",
    "\n",
    "![income](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/income.jpg?raw=true)\n",
    "\n",
    "***Q3: Can we predict the race distribution?***\n",
    "\n",
    "![race](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/race.jpg?raw=true)\n",
    "\n",
    "***Q4: Can we predict voting patterns?***\n",
    "\n",
    "![vote](https://github.com/arnauddyevre/Python-for-Social-Scientists/blob/master/introduction%20to%20machine%20learning/figures/vote.jpg?raw=true)\n",
    "\n",
    "We study below the techniques behind their success in predicting important socio-demographic variables with such precision, and geographical granularity: all of them fall in the class of **supervised learning** algorithms.\n",
    "\n",
    "### 2. Supervised learning\n",
    "\n",
    "> Supervised learning is the machine learning task of inferring a function from labeled training data.\n",
    "\n",
    "With supervised learning problems, we observe both the features $X_i$ of an observation and its label, or outcome $Y_i$.\n",
    "\n",
    "#### 2.1. Mathematical definition\n",
    "\n",
    "Mathematically, it means there is an unobservable function $f: X_i \\rightarrow Y_i$ where  $X_i$ is the input, $Y_i$ is the label.\n",
    "Given the data $(X_1, Y_1), \\dots, (X_n, Y_n)$, we want to use an algorithm to output an estimate of the function $\\hat{f}$.\n",
    "\n",
    "#### 2.2. Basic ideas\n",
    "\n",
    "**Training, testing data**\n",
    "\n",
    "Usually, we will divide the data set into 3 parts **[2 parts right? or is there a third missing below?]**: \n",
    "\n",
    "* _training data_ : learn the parameters of the model (sometimes also called hyperparameters **[TBC: When are these parameters called hyperparameters?]**)\n",
    "* _testing data_ : estimate the performance of the model\n",
    "\n",
    "**Prediction accuracy**\n",
    "\n",
    "Supervised learning can be divided into two catergories based on the output of this function:\n",
    "\n",
    "* regression problem: $Y \\in \\mathbb{R}$\n",
    "* classfication problem: $Y \\in \\{0, 1, \\dots, n\\}$\n",
    "\n",
    "In machine learning, the prediction accuaracy is the mean squared error between the predicted values and the real values in the regression problem and the classification correctness rate for the classification problem. **[TBC: we need to add formulas for the mean-squared error and the classification correctness rate here]**\n",
    "\n",
    "**Generalisation error and overfitting**\n",
    "\n",
    "**[an explanation would be welcome here, the picture seems to be in vacuum here. Also, we need to make sure that pictures are properly referenced. A source should be added]**\n",
    "\n",
    "![overfitting](https://raw.githubusercontent.com/DS-100/textbook/master/assets/feature_train_test_error.png)\n",
    "\n",
    "Machine learning is a broad term, that encompass many techniques:\n",
    "* nearest neighbor and clustering algorithms\n",
    "* naive Bayes\n",
    "* kernel functions\n",
    "* support vector machine\n",
    "* random forest\n",
    "* (stochastic) gradient descent\n",
    "* gradient boosting machine\n",
    "* neural network (aka deep learning)\n",
    "\n",
    "All these methods can be found in `sklearn` package which is the most popular machine learning packages in Python.\n",
    "\n",
    "Anaconda already has the `sklearn` module installed. Hence to use it, we simply need to import it at the beginning of our Notebook.\n",
    "\n",
    "### 3. Typical machine learning algorithms in `sklearn`\n",
    "\n",
    "**3.1 Data set**\n",
    "\n",
    "We use the `Guerry` data set from `statsmodels` to study a regression problem: can we predict the sales of the lottery? The detailed information about the data set can be found [here](https://rdata.pmagunia.com/dataset/r-dataset-package-histdata-guerry) **[Similarly, it would be useful to give afew words of explanations about this dataset and where it comes from. Or why you chose it.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dateutil 2.5.0 is the minimum required version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3da01b496302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Guerry\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HistData\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# remove all the non-numerical columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python2.7/site-packages/statsmodels/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimplefilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from statsmodels.tools.sm_exceptions import (ConvergenceWarning, CacheWriteWarning,\n\u001b[0m\u001b[1;32m     11\u001b[0m                                              IterationLimitWarning, InvalidTestWarning)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python2.7/site-packages/statsmodels/tools/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python2.7/site-packages/statsmodels/tools/tools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvdvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebuse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# numpy compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python2.7/site-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdateutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2.5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dateutil 2.5.0 is the minimum required version'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdateutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_date_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0mparse_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_date_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dateutil 2.5.0 is the minimum required version"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "dat = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n",
    "# remove all the non-numerical columns\n",
    "dat.drop([\"dept\", \"Region\", \"Department\", \"MainCity\"], axis=1, inplace=True)\n",
    "# list of the variables\n",
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crime_pers</th>\n",
       "      <th>Crime_prop</th>\n",
       "      <th>Literacy</th>\n",
       "      <th>Donations</th>\n",
       "      <th>Infants</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Commerce</th>\n",
       "      <th>Clergy</th>\n",
       "      <th>Crime_parents</th>\n",
       "      <th>Infanticide</th>\n",
       "      <th>Donation_clergy</th>\n",
       "      <th>Lottery</th>\n",
       "      <th>Desertion</th>\n",
       "      <th>Instruction</th>\n",
       "      <th>Prostitutes</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Area</th>\n",
       "      <th>Pop1831</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19754.406977</td>\n",
       "      <td>7843.058140</td>\n",
       "      <td>39.255814</td>\n",
       "      <td>7075.546512</td>\n",
       "      <td>19049.906977</td>\n",
       "      <td>36522.604651</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>42.802326</td>\n",
       "      <td>43.430233</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.511628</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.127907</td>\n",
       "      <td>141.872093</td>\n",
       "      <td>207.953140</td>\n",
       "      <td>6146.988372</td>\n",
       "      <td>378.628721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7504.703073</td>\n",
       "      <td>3051.352839</td>\n",
       "      <td>17.364051</td>\n",
       "      <td>5834.595216</td>\n",
       "      <td>8820.233546</td>\n",
       "      <td>31312.532649</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>25.028370</td>\n",
       "      <td>24.999549</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.948297</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.969982</td>\n",
       "      <td>24.799809</td>\n",
       "      <td>520.969318</td>\n",
       "      <td>109.320837</td>\n",
       "      <td>1398.246620</td>\n",
       "      <td>148.777230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2199.000000</td>\n",
       "      <td>1368.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1246.000000</td>\n",
       "      <td>2660.000000</td>\n",
       "      <td>3460.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>129.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14156.250000</td>\n",
       "      <td>5933.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3446.750000</td>\n",
       "      <td>14299.750000</td>\n",
       "      <td>15463.000000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>121.383000</td>\n",
       "      <td>5400.750000</td>\n",
       "      <td>283.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18748.500000</td>\n",
       "      <td>7595.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>5020.000000</td>\n",
       "      <td>17141.500000</td>\n",
       "      <td>26743.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>200.616000</td>\n",
       "      <td>6070.500000</td>\n",
       "      <td>346.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25937.500000</td>\n",
       "      <td>9182.250000</td>\n",
       "      <td>51.750000</td>\n",
       "      <td>9446.750000</td>\n",
       "      <td>22682.250000</td>\n",
       "      <td>44057.500000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>113.750000</td>\n",
       "      <td>289.670500</td>\n",
       "      <td>6816.500000</td>\n",
       "      <td>444.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>37014.000000</td>\n",
       "      <td>20235.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>37015.000000</td>\n",
       "      <td>62486.000000</td>\n",
       "      <td>163241.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4744.000000</td>\n",
       "      <td>539.213000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>989.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Crime_pers    Crime_prop   Literacy     Donations       Infants  \\\n",
       "count     86.000000     86.000000  86.000000     86.000000     86.000000   \n",
       "mean   19754.406977   7843.058140  39.255814   7075.546512  19049.906977   \n",
       "std     7504.703073   3051.352839  17.364051   5834.595216   8820.233546   \n",
       "min     2199.000000   1368.000000  12.000000   1246.000000   2660.000000   \n",
       "25%    14156.250000   5933.000000  25.000000   3446.750000  14299.750000   \n",
       "50%    18748.500000   7595.000000  38.000000   5020.000000  17141.500000   \n",
       "75%    25937.500000   9182.250000  51.750000   9446.750000  22682.250000   \n",
       "max    37014.000000  20235.000000  74.000000  37015.000000  62486.000000   \n",
       "\n",
       "            Suicides     Wealth   Commerce     Clergy  Crime_parents  \\\n",
       "count      86.000000  86.000000  86.000000  86.000000      86.000000   \n",
       "mean    36522.604651  43.500000  42.802326  43.430233      43.500000   \n",
       "std     31312.532649  24.969982  25.028370  24.999549      24.969982   \n",
       "min      3460.000000   1.000000   1.000000   1.000000       1.000000   \n",
       "25%     15463.000000  22.250000  21.250000  22.250000      22.250000   \n",
       "50%     26743.500000  43.500000  42.500000  43.500000      43.500000   \n",
       "75%     44057.500000  64.750000  63.750000  64.750000      64.750000   \n",
       "max    163241.000000  86.000000  86.000000  86.000000      86.000000   \n",
       "\n",
       "       Infanticide  Donation_clergy    Lottery  Desertion  Instruction  \\\n",
       "count    86.000000        86.000000  86.000000  86.000000    86.000000   \n",
       "mean     43.511628        43.500000  43.500000  43.500000    43.127907   \n",
       "std      24.948297        24.969982  24.969982  24.969982    24.799809   \n",
       "min       1.000000         1.000000   1.000000   1.000000     1.000000   \n",
       "25%      22.250000        22.250000  22.250000  22.250000    23.250000   \n",
       "50%      43.500000        43.500000  43.500000  43.500000    41.500000   \n",
       "75%      64.750000        64.750000  64.750000  64.750000    64.750000   \n",
       "max      86.000000        86.000000  86.000000  86.000000    86.000000   \n",
       "\n",
       "       Prostitutes    Distance          Area     Pop1831  \n",
       "count    86.000000   86.000000     86.000000   86.000000  \n",
       "mean    141.872093  207.953140   6146.988372  378.628721  \n",
       "std     520.969318  109.320837   1398.246620  148.777230  \n",
       "min       0.000000    0.000000    762.000000  129.100000  \n",
       "25%       6.000000  121.383000   5400.750000  283.005000  \n",
       "50%      33.000000  200.616000   6070.500000  346.165000  \n",
       "75%     113.750000  289.670500   6816.500000  444.407500  \n",
       "max    4744.000000  539.213000  10000.000000  989.940000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dat['Lottery']\n",
    "dat.drop('Lottery', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly divide the data set into `train_X`, `train_Y` , `test_X` and `test_Y` where the size of training, validation and test  $\\sim 4:1$ **[We would need an explanation of how to divide the dataset in training and test data, what's the rule of thumb or the heuristic here?]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 70 training data\n",
      "There are 16 test data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "test_X = [] \n",
    "test_Y = []\n",
    "\n",
    "for idx, row in dat.iterrows():\n",
    "    r = np.random.randint(5)\n",
    "    if  r < 4:\n",
    "        train_X.append(row.values)\n",
    "        train_Y.append(Y[idx])\n",
    "    else:\n",
    "        test_X.append(row.values)\n",
    "        test_Y.append(Y[idx])\n",
    "\n",
    "print(\"There are \" + str(len(train_Y)) + \" training data\")\n",
    "print(\"There are \" + str(len(test_Y)) + \" test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2. Linear model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 428.82\n",
      "Variance score: 0.42\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(train_X, train_Y)\n",
    "ols_pred_Y = ols.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, ols_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, ols_pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[We need an explanation of what LASSO does and its mathematical expression]**\n",
    "\n",
    "LASSO minimises the sum of squared residuals and a penalty term that increases with the dimension of the regression coefficient. In a sense, LASSO penalises model complexity.\n",
    "\n",
    "$$\\arg \\min _{\\beta} \\sum_{i=1}^{N}\\left(Y_{i}-\\beta^{\\top} X_{i}\\right)^{2}+\\lambda\\left(\\|\\beta\\|_{q}\\right)^{1 /q}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 524.26\n",
      "Variance score: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "lasso = LassoCV(cv=5, random_state=0)\n",
    "lasso.fit(train_X, train_Y)\n",
    "lasso_pred_Y = lasso.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, lasso_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, lasso_pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3. $k$-Nearest Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 587.54\n",
      "Variance score: 0.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "n_neighbors = 10\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors, weights='uniform')\n",
    "knn.fit(train_X, train_Y)\n",
    "knn_pred_Y = knn.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, knn_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, knn_pred_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 771.00\n",
      "Variance score: -0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVR()\n",
    "clf.fit(train_X, train_Y)\n",
    "clf_pred_Y = clf.predict(test_X)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test_Y, clf_pred_Y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(test_Y, clf_pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[I think this Notebook is a bit too short...]**. It would be great if it were expanded with more techniques/methods. We should also add a class exercise with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**Gebru, T., Krause, J., Wang, Y., Chen, D., Deng, J., Aiden, E. L., & Fei-Fei, L.** (2017). Using deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States. *Proceedings of the National Academy of Sciences*, 114(50), 13108-13113.\n",
    "\n",
    "**Athey, S., & Imbens, G. W.** (2019). Machine learning methods that economists should know about. *Annual Review of Economics*, 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/keras/basic_text_classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
